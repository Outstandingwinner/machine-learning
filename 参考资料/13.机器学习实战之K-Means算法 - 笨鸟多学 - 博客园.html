<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="referrer" content="origin" />
    <title>机器学习实战之K-Means算法 - 笨鸟多学 - 博客园</title>
<meta property="og:description" content="一，引言 先说个K-means算法很高大上的用处，来开始新的算法学习。我们都知道每一届的美国总统大选，那叫一个竞争激烈。可以说，谁拿到了各个州尽可能多的选票，谁选举获胜的几率就会非常大。有人会说，这跟" />
    <link type="text/css" rel="stylesheet" href="/bundles/blog-common.css?v=-duj5vpGTntb85GJoM3iRI972XwWcI-j8zmqDzyfu2w1"/>
<link id="MainCss" type="text/css" rel="stylesheet" href="/skins/LessIsMoreRight/bundle-LessIsMoreRight.css?v=XnHJrmT6UJMtyGfeJjiTUm7BxKWcwdJrxKsGy7z3YZ81"/>
<link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/LessIsMoreRight/bundle-LessIsMoreRight-mobile.css?v=qM7K821bC07wdAlQFyWU6RMqTAL_q0KcZ4T6U6MOu941"/>
    <link title="RSS" type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/zy230530/rss"/>
    <link title="RSD" type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/zy230530/rsd.xml"/>
<link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/zy230530/wlwmanifest.xml"/>
    <script src="//common.cnblogs.com/scripts/jquery-2.2.0.min.js"></script>
    <script>var currentBlogId=343941;var currentBlogApp='zy230530',cb_enable_mathjax=false;var isLogined=false;</script>
    <script src="/bundles/blog-common.js?v=75GlRjvNr9aYgWttsJIxQDp4deiGqNQyDe6Io4CHSa81" type="text/javascript"></script>
</head>
<body>
<a name="top"></a>


<div id="home">
<div id="header">
	<div id="blogTitle">
		
<!--done-->
<div class="title"><a id="Header1_HeaderTitle" class="headermaintitle" href="https://www.cnblogs.com/zy230530/">笨鸟多学</a></div>
<div class="subtitle"></div>



		
	</div><!--end: blogTitle 博客的标题和副标题 -->
	<div id="navigator">
		
<ul id="navList">
<li id="nav_sitehome"><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">博客园</a></li>
<li id="nav_myhome"><a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/zy230530/">首页</a></li>
<li id="nav_newpost"><a id="blog_nav_newpost" class="menu" rel="nofollow" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">新随笔</a></li>
<li id="nav_contact"><a id="blog_nav_contact" class="menu" rel="nofollow" href="https://msg.cnblogs.com/send/%E7%AC%A8%E9%B8%9F%E5%A4%9A%E5%AD%A6">联系</a></li>
<li id="nav_rss"><a id="blog_nav_rss" class="menu" href="https://www.cnblogs.com/zy230530/rss">订阅</a>
<!--<a id="blog_nav_rss_image" class="aHeaderXML" href="https://www.cnblogs.com/zy230530/rss"><img src="//www.cnblogs.com/images/xml.gif" alt="订阅" /></a>--></li>
<li id="nav_admin"><a id="blog_nav_admin" class="menu" rel="nofollow" href="https://i.cnblogs.com/">管理</a></li>
</ul>

		<div class="blogStats">
			
			<div id="blog_stats">
<!--done-->
随笔-27&nbsp;
文章-5&nbsp;
评论-38&nbsp;
</div>
			
		</div><!--end: blogStats -->
	</div><!--end: navigator 博客导航栏 -->
</div><!--end: header 头部 -->
<div id="main">
	<div id="mainContent">
	<div class="forFlow">
		
        <div id="post_detail">
<!--done-->
<div id="topics">
	<div class = "post">
		<h1 class = "postTitle">
			<a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/zy230530/p/7029025.html">机器学习实战之K-Means算法</a>
		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body"><p>一，引言</p>
<p>　　先说个K-means算法很高大上的用处，来开始新的算法学习。我们都知道每一届的美国总统大选，那叫一个竞争激烈。可以说，谁拿到了各个州尽可能多的选票，谁选举获胜的几率就会非常大。有人会说，这跟K-means算法有什么关系？当然，如果哪一届的总统竞选，某一位候选人是绝对的众望所归，那自然能以压倒性优势竞选成功，那么我们的k-means算法还真用不上。但是，我们应该知道2004年的总统大选中，候选人的得票数非常接近，接近到什么程度呢？如果1%的选民将手中的选票投向任何一位候选人，都直接决定了总统的归属。那么这个时候，这1%的选民手中的选票就非常关键，因为他们的选票将直接对选举结果产生非常大的影响，所以，如果能够妥善加以引导和吸引，那么这很少的一部分选民还是极有可能会转换立场的。那么如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们呢？</p>
<p>　　答案就是聚类，这就要说到本次要讲到的K-means算法了。通过收集用户的信息，可以同时收集用户满意和不满意的信息；然后将这些信息输入到聚类算法中，就会得到很多的簇；接着，对聚类结果中的每一个簇(最好是最大簇)，精心构造能吸引该簇选民的信息，加以引导；最后，再开展竞选活动并观察上述做法是否有效。而，一旦算法有效，那么就会对选举结果产生非常大的影响，甚至，直接决定了最后的总统归属。</p>
<p>　　　　可见，聚类算法是一个非常了不起的算法。下面，我们就正式开始今天的新算法，K-means聚类算法。</p>
<p>二，K-means聚类算法</p>
<p>1&nbsp;K-means算法的相关描述</p>
<p>　　聚类是一种无监督的学习，它将相似的对象归到同一簇中。聚类的方法几乎可以应用所有对象，簇内的对象越相似，聚类的效果就越好。K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。</p>
<p>　　聚类和分类最大的不同在于，分类的目标是事先已知的，而聚类则不一样，聚类事先不知道目标变量是什么，类别没有像分类那样被预先定义出来，所以，聚类有时也叫无监督学习。</p>
<p>　　聚类分析试图将相似的对象归入同一簇，将不相似的对象归为不同簇，那么，显然需要一种合适的相似度计算方法，我们已知的有很多相似度的计算方法，比如欧氏距离，余弦距离，汉明距离等。事实上，我们应该根据具体的应用来选取合适的相似度计算方法。</p>
<p>　　当然，任何一种算法都有一定的缺陷，没有一种算法时完美的，有的只是人类不断追求完美，不断创新的意志。K-means算法也有它的缺陷，但是我们可以通过一些后处理来得到更好的聚类结果，这些在后面都会一一降到。</p>
<p>　　K-means算法虽然比较容易实现，但是其可能收敛到局部最优解，且在大规模数据集上收敛速度相对较慢。</p>
<p>2&nbsp;K-means算法的工作流程</p>
<p>　　首先，随机确定k个初始点的质心；然后将数据集中的每一个点分配到一个簇中，即为每一个点找到距其最近的质心，并将其分配给该质心所对应的簇；该步完成后，每一个簇的质心更新为该簇所有点的平均值。伪代码如下：</p>
<div class="cnblogs_code">
<pre><span style="color: #000000">创建k个点作为起始质心，可以随机选择(位于数据边界内)
　　当任意一个点的簇分配结果发生改变时
　　　　对数据集中每一个点
　　　　　　　　对每个质心
　　　　　　　　　　计算质心与数据点之间的距离
　　　　　　　　将数据点分配到距其最近的簇
　　　　对每一个簇，计算簇中所有点的均值并将均值作为质心</span></pre>
</div>
<p>　　再看实际的代码：</p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000">导入numpy库</span>
<span style="color: #0000ff">from</span> numpy <span style="color: #0000ff">import</span> *
<span style="color: #008000">#</span><span style="color: #008000">K-均值聚类辅助函数</span>

<span style="color: #008000">#</span><span style="color: #008000">文本数据解析函数</span>
<span style="color: #0000ff">def</span> numpy <span style="color: #0000ff">import</span> *<span style="color: #000000">
    dataMat</span>=<span style="color: #000000">[]
    fr</span>=<span style="color: #000000">open(fileName)
    </span><span style="color: #0000ff">for</span> line <span style="color: #0000ff">in</span><span style="color: #000000"> fr.readlines():
        curLine</span>=line.strip().split(<span style="color: #800000">'</span><span style="color: #800000">\t</span><span style="color: #800000">'</span><span style="color: #000000">)
        </span><span style="color: #008000">#</span><span style="color: #008000">将每一行的数据映射成float型</span>
        fltLine=<span style="color: #000000">map(float,curLine)
        dataMat.append(fltLine)
    </span><span style="color: #0000ff">return</span><span style="color: #000000"> dataMat

</span><span style="color: #008000">#</span><span style="color: #008000">数据向量计算欧式距离    </span>
<span style="color: #0000ff">def</span><span style="color: #000000"> distEclud(vecA,vecB):
    </span><span style="color: #0000ff">return</span> sqrt(sum(power(vecA-vecB,2<span style="color: #000000">)))

</span><span style="color: #008000">#</span><span style="color: #008000">随机初始化K个质心(质心满足数据边界之内)</span>
<span style="color: #0000ff">def</span><span style="color: #000000"> randCent(dataSet,k):
    </span><span style="color: #008000">#</span><span style="color: #008000">得到数据样本的维度</span>
    n=shape(dataSet)[1<span style="color: #000000">]
    </span><span style="color: #008000">#</span><span style="color: #008000">初始化为一个(k,n)的矩阵</span>
    centroids=<span style="color: #000000">mat(zeros((k,n)))
    </span><span style="color: #008000">#</span><span style="color: #008000">遍历数据集的每一维度</span>
    <span style="color: #0000ff">for</span> j <span style="color: #0000ff">in</span><span style="color: #000000"> range(n):
        </span><span style="color: #008000">#</span><span style="color: #008000">得到该列数据的最小值</span>
        minJ=<span style="color: #000000">min(dataSet[:,j])
        </span><span style="color: #008000">#</span><span style="color: #008000">得到该列数据的范围(最大值-最小值)</span>
        rangeJ=float(max(dataSet[:,j])-<span style="color: #000000">minJ)
        </span><span style="color: #008000">#</span><span style="color: #008000">k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值</span>
        centroids[:,j]=minJ+rangeJ*random.rand(k,1<span style="color: #000000">)
    </span><span style="color: #008000">#</span><span style="color: #008000">返回初始化得到的k个质心向量</span>
    <span style="color: #0000ff">return</span><span style="color: #000000"> centroids
    
</span><span style="color: #008000">#</span><span style="color: #008000">k-均值聚类算法</span><span style="color: #008000">
#</span><span style="color: #008000">@dataSet:聚类数据集</span><span style="color: #008000">
#</span><span style="color: #008000">@k:用户指定的k个类</span><span style="color: #008000">
#</span><span style="color: #008000">@distMeas:距离计算方法，默认欧氏距离distEclud()</span><span style="color: #008000">
#</span><span style="color: #008000">@createCent:获得k个质心的方法，默认随机获取randCent()</span>
<span style="color: #0000ff">def</span> kMeans(dataSet,k,distMeas=distEclud,createCent=<span style="color: #000000">randCent):
    </span><span style="color: #008000">#</span><span style="color: #008000">获取数据集样本数</span>
    m=<span style="color: #000000">shape(dataSet)[0]
    </span><span style="color: #008000">#</span><span style="color: #008000">初始化一个(m,2)的矩阵</span>
    clusterAssment=mat(zeros((m,2<span style="color: #000000">)))
    </span><span style="color: #008000">#</span><span style="color: #008000">创建初始的k个质心向量</span>
    centroids=<span style="color: #000000">createCent(dataSet,k)
    </span><span style="color: #008000">#</span><span style="color: #008000">聚类结果是否发生变化的布尔类型</span>
    clusterChanged=<span style="color: #000000">True
    </span><span style="color: #008000">#</span><span style="color: #008000">只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不变化</span>
    <span style="color: #0000ff">while</span><span style="color: #000000"> clusterChanged:
        </span><span style="color: #008000">#</span><span style="color: #008000">聚类结果变化布尔类型置为false</span>
        clusterChanged=<span style="color: #000000">False
        </span><span style="color: #008000">#</span><span style="color: #008000">遍历数据集每一个样本向量</span>
        <span style="color: #0000ff">for</span> i <span style="color: #0000ff">in</span><span style="color: #000000"> range(m):
            </span><span style="color: #008000">#</span><span style="color: #008000">初始化最小距离最正无穷；最小距离对应索引为-1</span>
            minDist=inf;minIndex=-1
            <span style="color: #008000">#</span><span style="color: #008000">循环k个类的质心</span>
            <span style="color: #0000ff">for</span> j <span style="color: #0000ff">in</span><span style="color: #000000"> range(k):
                </span><span style="color: #008000">#</span><span style="color: #008000">计算数据点到质心的欧氏距离</span>
                distJI=<span style="color: #000000">distMeas(centroids[j,:],dataSet[i,:])
                </span><span style="color: #008000">#</span><span style="color: #008000">如果距离小于当前最小距离</span>
                <span style="color: #0000ff">if</span> distJI&lt;<span style="color: #000000">minDist:
                    </span><span style="color: #008000">#</span><span style="color: #008000">当前距离定为当前最小距离；最小距离对应索引对应为j(第j个类)</span>
                    minDist=distJI;minIndex=<span style="color: #000000">j
        </span><span style="color: #008000">#</span><span style="color: #008000">当前聚类结果中第i个样本的聚类结果发生变化：布尔类型置为true，继续聚类算法</span>
        <span style="color: #0000ff">if</span> clusterAssment[i,0] !=minIndex:clusterChanged=<span style="color: #000000">True
        </span><span style="color: #008000">#</span><span style="color: #008000">更新当前变化样本的聚类结果和平方误差</span>
        clusterAssment[i,:]=minIndex,minDist**2
    <span style="color: #008000">#</span><span style="color: #008000">打印k-均值聚类的质心</span>
    <span style="color: #0000ff">print</span><span style="color: #000000"> centroids
    </span><span style="color: #008000">#</span><span style="color: #008000">遍历每一个质心</span>
    <span style="color: #0000ff">for</span> cent <span style="color: #0000ff">in</span><span style="color: #000000"> range(k):
        </span><span style="color: #008000">#</span><span style="color: #008000">将数据集中所有属于当前质心类的样本通过条件过滤筛选出来</span>
        ptsInClust=dataSet[nonzero(clusterAssment[:,0].A==<span style="color: #000000">cent)[0]]
        </span><span style="color: #008000">#</span><span style="color: #008000">计算这些数据的均值（axis=0：求列的均值），作为该类质心向量</span>
        centroids[cent,:]=mean(ptsInClust,axis=<span style="color: #000000">0)
    </span><span style="color: #008000">#</span><span style="color: #008000">返回k个聚类，聚类结果及误差</span>
    <span style="color: #0000ff">return</span> centroids,clusterAssment</pre>
</div>
<p>　　需要说明的是，在算法中，相似度的计算方法默认的是欧氏距离计算，当然也可以使用其他相似度计算函数，比如余弦距离；算法中，k个类的初始化方式为随机初始化，并且初始化的质心必须在整个数据集的边界之内，这可以通过找到数据集每一维的最大值和最小值；然后最小值+取值范围*0到1的随机数，来确保随机点在数据边界之内。</p>
<p>　　在实际的K-means算法中，采用计算质心-分配-重新计算质心的方式反复迭代，算法停止的条件是，当然数据集所有的点分配的距其最近的簇不在发生变化时，就停止分配，更新所有簇的质心后，返回k个类的质心(一般是向量的形式)组成的质心列表，以及存储各个数据点的分类结果和误差距离的平方的二维矩阵。</p>
<p>　　上面返回的结果中，之所以存储每个数据点距离其质心误差距离平方，是便于后续的算法预处理。因为K-means算法采取的是随机初始化k个簇的质心的方式，因此聚类效果又可能陷入局部最优解的情况，局部最优解虽然效果不错，但不如全局最优解的聚类效果更好。所以，后续会在算法结束后，采取相应的后处理，使算法跳出局部最优解，达到全局最优解，获得最好的聚类效果。</p>
<p>　　可以看一个聚类的例子:</p>
<p><img src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616163641071-429406662.jpg" alt="" width="489" height="30"></p>
<p><img src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616165329306-821394393.jpg" alt="" width="494" height="120"></p>
<p><img src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616165425478-272755369.jpg" alt="" width="493" height="139"></p>
<p><img style="display: block; margin-left: auto; margin-right: auto" src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616165525978-29354842.jpg" alt="" width="433" height="338"></p>
<p>3 后处理提高聚类性能</p>
<p><img style="display: block; margin-left: auto; margin-right: auto" src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616173445853-1467088044.jpg" alt="" width="417" height="324">　　有时候当我们观察聚类的结果图时，发现聚类的效果没有那么好，如上图所示，K-means算法在k值选取为3时的聚类结果，我们发现，算法能够收敛但效果较差。显然，这种情况的原因是，算法收敛到了局部最小值，而并不是全局最小值，局部最小值显然没有全局最小值的结果好。</p>
<p>　　那么，既然知道了算法已经陷入了局部最小值，如何才能够进一步提升K-means算法的效果呢？</p>
<p>　　一种用于度量聚类效果的指标是SSE，即误差平方和， 为所有簇中的全部数据点到簇中心的误差距离的平方累加和。SSE的值如果越小，表示数据点越接近于它们的簇中心，即质心，聚类效果也越好。因为，对误差取平方后，就会更加重视那些远离中心的数据点。</p>
<p>　　显然，我们知道了一种改善聚类效果的做法就是降低SSE，那么如何在保持簇数目不变的情况下提高簇的质量呢？</p>
<p>　　一种方法是：我们可以将具有最大SSE值得簇划分为两个簇（因为，SSE最大的簇一般情况下，意味着簇内的数据点距离簇中心较远），具体地，可以将最大簇包含的点过滤出来并在这些点上运行K-means算法，其中k设为2.</p>
<p>　　同时，当把最大的簇(上图中的下半部分)分为两个簇之后，为了保证簇的数目是不变的，我们可以再合并两个簇。具体地：</p>
<p>　　一方面我们可以合并两个最近的质心所对应的簇，即计算所有质心之间的距离，合并质心距离最近的两个质心所对应的簇。</p>
<p>　　另一方面，我们可以合并两个使得SSE增幅最小的簇，显然，合并两个簇之后SSE的值会有所上升，那么为了最好的聚类效果，应该尽可能使总的SSE值小，所以就选择合并两个簇后SSE涨幅最小的簇。具体地，就是计算合并任意两个簇之后的总得SSE，选取合并后最小的SSE对应的两个簇进行合并。这样，就可以满足簇的数目不变。</p>
<p>　　上面，是对已经聚类完成的结果进行改善的方法，在不改变k值的情况下，上述方法能够起到一定的作用，会使得聚类效果得到一定的改善。那么，下面要讲到的是一种克服算法收敛于局部最小值问题的K-means算法。即二分k-均值算法。</p>
<p>三，二分K-means算法</p>
<p>　　二分K-means算法首先将所有点作为一个簇，然后将簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇取决于对其进行划分是否能够最大程度的降低SSE的值。上述划分过程不断重复，直至划分的簇的数目达到用户指定的值为止。</p>
<p>　　二分K-means算法的伪代码如下：</p>
<div class="cnblogs_code">
<pre><span style="color: #000000">将所有点看成一个簇
当簇数目小于k时
对于每一个簇
    计算总误差
    在给定的簇上面进行k</span>-均值聚类（k=2<span style="color: #000000">）
    计算将该簇一分为二之后的总误差
选择使得总误差最小的簇进行划分</span></pre>
</div>
<p>　　当然，也可以选择SSE最大的簇进行划分，知道簇数目达到用户指定的数目为止。下面看具体的代码：</p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000">二分K-均值聚类算法</span><span style="color: #008000">
#</span><span style="color: #008000">@dataSet:待聚类数据集</span><span style="color: #008000">
#</span><span style="color: #008000">@k：用户指定的聚类个数</span><span style="color: #008000">
#</span><span style="color: #008000">@distMeas:用户指定的距离计算方法，默认为欧式距离计算</span>
<span style="color: #0000ff">def</span> biKmeans(dataSet,k,distMeas=<span style="color: #000000">distEclud):
    </span><span style="color: #008000">#</span><span style="color: #008000">获得数据集的样本数</span>
    m=<span style="color: #000000">shape(dataSet)[0]
    </span><span style="color: #008000">#</span><span style="color: #008000">初始化一个元素均值0的(m,2)矩阵</span>
    clusterAssment=mat(zeros((m,2<span style="color: #000000">)))
    </span><span style="color: #008000">#</span><span style="color: #008000">获取数据集每一列数据的均值，组成一个长为列数的列表</span>
    centroid0=mean(dataSet,axis=<span style="color: #000000">0).tolist()[0]
    </span><span style="color: #008000">#</span><span style="color: #008000">当前聚类列表为将数据集聚为一类</span>
    centList=<span style="color: #000000">[centroid0]
    </span><span style="color: #008000">#</span><span style="color: #008000">遍历每个数据集样本</span>
    <span style="color: #0000ff">for</span> j <span style="color: #0000ff">in</span><span style="color: #000000"> range(m):
        </span><span style="color: #008000">#</span><span style="color: #008000">计算当前聚为一类时各个数据点距离质心的平方距离</span>
        clusterAssment[j,1]=distMeas(mat(centroid0),dataSet[j,:])**2
    <span style="color: #008000">#</span><span style="color: #008000">循环，直至二分k-均值达到k类为止</span>
    <span style="color: #0000ff">while</span> (len(centList)&lt;<span style="color: #000000">k):
        </span><span style="color: #008000">#</span><span style="color: #008000">将当前最小平方误差置为正无穷</span>
        lowerSSE=<span style="color: #000000">inf
        </span><span style="color: #008000">#</span><span style="color: #008000">遍历当前每个聚类</span>
        <span style="color: #0000ff">for</span> i <span style="color: #0000ff">in</span><span style="color: #000000"> range(len(centList)):
            </span><span style="color: #008000">#</span><span style="color: #008000">通过数组过滤筛选出属于第i类的数据集合</span>
            ptsInCurrCluster=<span style="color: #000000">\
                dataSet[nonzero(clusterAssment[:,0].A</span>==<span style="color: #000000">i)[0],:]
            </span><span style="color: #008000">#</span><span style="color: #008000">对该类利用二分k-均值算法进行划分，返回划分后结果，及误差</span>
            centroidMat,splitClustAss=<span style="color: #000000">\
                kMeans(ptsInCurrCluster,</span>2<span style="color: #000000">,distMeas)
            </span><span style="color: #008000">#</span><span style="color: #008000">计算该类划分后两个类的误差平方和</span>
            sseSplit=sum(splitClustAss[:,1<span style="color: #000000">])
            </span><span style="color: #008000">#</span><span style="color: #008000">计算数据集中不属于该类的数据的误差平方和</span>
            sseNotSplit=<span style="color: #000000">\
                sum(clusterAssment[nonzero(clusterAssment[:,0].A</span>!=i)[0],1<span style="color: #000000">])
            </span><span style="color: #008000">#</span><span style="color: #008000">打印这两项误差值</span>
            <span style="color: #0000ff">print</span>(<span style="color: #800000">'</span><span style="color: #800000">sseSplit,and notSplit:</span><span style="color: #800000">'</span>,%<span style="color: #000000">(sseSplit,sseNotSplit))
            </span><span style="color: #008000">#</span><span style="color: #008000">划分第i类后总误差小于当前最小总误差</span>
            <span style="color: #0000ff">if</span>(sseSplit+sseNotSplit)&lt;<span style="color: #000000">lowerSSE:
                </span><span style="color: #008000">#</span><span style="color: #008000">第i类作为本次划分类</span>
                bestCentToSplit=<span style="color: #000000">i
                </span><span style="color: #008000">#</span><span style="color: #008000">第i类划分后得到的两个质心向量</span>
                bestNewCents=<span style="color: #000000">centroidMat
                </span><span style="color: #008000">#</span><span style="color: #008000">复制第i类中数据点的聚类结果即误差值</span>
                bestClustAss=<span style="color: #000000">splitClustAss.copy()
                </span><span style="color: #008000">#</span><span style="color: #008000">将划分第i类后的总误差作为当前最小误差</span>
                lowerSSE=sseSplit+<span style="color: #000000">sseNotSplit
        </span><span style="color: #008000">#</span><span style="color: #008000">数组过滤筛选出本次2-均值聚类划分后类编号为1数据点，将这些数据点类编号变为</span>
        <span style="color: #008000">#</span><span style="color: #008000">当前类个数+1，作为新的一个聚类</span>
        bestClustAss[nonzero(bestClustAss[:,0].A==1)[0],0]=<span style="color: #000000">\    
                len(centList)
        </span><span style="color: #008000">#</span><span style="color: #008000">同理，将划分数据集中类编号为0的数据点的类编号仍置为被划分的类编号，使类编号</span>
        <span style="color: #008000">#</span><span style="color: #008000">连续不出现空缺</span>
        bestClustAss[nonzero(bestClustAss[:,0].A==0)[0],0]=<span style="color: #000000">\    
                bestCentToSplit
        </span><span style="color: #008000">#</span><span style="color: #008000">打印本次执行2-均值聚类算法的类</span>
        <span style="color: #0000ff">print</span>(<span style="color: #800000">'</span><span style="color: #800000">the bestCentToSplit is:</span><span style="color: #800000">'</span>,%<span style="color: #000000">bestCentToSplit)
        </span><span style="color: #008000">#</span><span style="color: #008000">打印被划分的类的数据个数</span>
        <span style="color: #0000ff">print</span>(<span style="color: #800000">'</span><span style="color: #800000">the len of bestClustAss is:</span><span style="color: #800000">'</span>,%<span style="color: #000000">(len(bestClustAss)))
        </span><span style="color: #008000">#</span><span style="color: #008000">更新质心列表中的变化后的质心向量</span>
        centList[bestCentToSplit]=<span style="color: #000000">bestNewCents[0,:]
        </span><span style="color: #008000">#</span><span style="color: #008000">添加新的类的质心向量</span>
        centList.append(bestNewCents[1<span style="color: #000000">,:])
        </span><span style="color: #008000">#</span><span style="color: #008000">更新clusterAssment列表中参与2-均值聚类数据点变化后的分类编号，及数据该类的误差平方</span>
        clusterAssment[nonzero(clusterAssment[:,0].A==<span style="color: #000000">\
                bestCentToSplit)[0],:]</span>=<span style="color: #000000">bestClustAss
        </span><span style="color: #008000">#</span><span style="color: #008000">返回聚类结果</span>
        <span style="color: #0000ff">return</span> mat(centList),clusterAssment</pre>
</div>
<p>　　在上述算法中，直到簇的数目达到k值，算法才会停止。在算法中通过将所有的簇进行划分，然后分别计算划分后所有簇的误差。选择使得总误差最小的那个簇进行划分。划分完成后，要更新簇的质心列表，数据点的分类结果及误差平方。具体地，假设划分的簇为m（m&lt;k)个簇中的第i个簇，那么这个簇分成的两个簇后，其中一个取代该被划分的簇，成为第i个簇，并计算该簇的质心；此外，将划分得到的另外一个簇，作为一个新的簇，成为第m+1个簇，并计算该簇的质心。此外，算法中还存储了各个数据点的划分结果和误差平方，此时也应更新相应的存储信息。这样，重复该过程，直至簇个数达到k。</p>
<p>　　通过上述算法，之前陷入局部最小值的的这些数据，经过二分K-means算法多次划分后，逐渐收敛到全局最小值，从而达到了令人满意的聚类效果。</p>
<p><img style="display: block; margin-left: auto; margin-right: auto" src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616182835165-1204921357.jpg" alt="" width="357" height="272"></p>
<p>四，示例：对地图上的点进行聚类</p>
<p>　　现在有一个存有70个地址和城市名的文本，而没有这些地点的距离信息。而我们想要对这些地点进行聚类，找到每个簇的质心地点，从而可以安排合理的行程，即质心之间选择交通工具抵达，而位于每个质心附近的地点就可以采取步行的方法抵达。显然，K-means算法可以为我们找到一种更加经济而且高效的出行方式。</p>
<p>1 通过地址信息获取相应的经纬度信息</p>
<p>　　那么，既然没有地点之间的距离信息，怎么计算地点之间的距离呢？又如何比较地点之间的远近呢？</p>
<p>　　我们手里只有各个地点的地址信息，那么如果有一个API，可以让我们输入地点信息，返回该地点的经度和纬度信息，那么我们就可以通过球面距离计算方法得到两个地点之间的距离了。而Yahoo！PlaceFinder API可以帮助我们实现这一目标。获取地点信息对应经纬度的代码如下：</p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000">Yahoo！PlaceFinder API</span><span style="color: #008000">
#</span><span style="color: #008000">导入urllib</span>
<span style="color: #0000ff">import</span><span style="color: #000000"> urllib
</span><span style="color: #008000">#</span><span style="color: #008000">导入json模块</span>
<span style="color: #0000ff">import</span><span style="color: #000000"> json

</span><span style="color: #008000">#</span><span style="color: #008000">利用地名，城市获取位置经纬度函数</span>
<span style="color: #0000ff">def</span><span style="color: #000000"> geoGrab(stAddress,city):
    </span><span style="color: #008000">#</span><span style="color: #008000">获取经纬度网址</span>
    apiStem=<span style="color: #800000">'</span><span style="color: #800000">http://where.yahooapis.com/geocode?</span><span style="color: #800000">'</span>
    <span style="color: #008000">#</span><span style="color: #008000">初始化一个字典，存储相关参数</span>
    params=<span style="color: #000000">{}
    </span><span style="color: #008000">#</span><span style="color: #008000">返回类型为json</span>
    params[<span style="color: #800000">'</span><span style="color: #800000">flags</span><span style="color: #800000">'</span>]=<span style="color: #800000">'</span><span style="color: #800000">J</span><span style="color: #800000">'</span>
    <span style="color: #008000">#</span><span style="color: #008000">参数appid</span>
    params[<span style="color: #800000">'</span><span style="color: #800000">appid</span><span style="color: #800000">'</span>]=<span style="color: #800000">'</span><span style="color: #800000">ppp68N8t</span><span style="color: #800000">'</span>
    <span style="color: #008000">#</span><span style="color: #008000">参数地址位置信息</span>
    params[<span style="color: #800000">'</span><span style="color: #800000">location</span><span style="color: #800000">'</span>]=(<span style="color: #800000">'</span><span style="color: #800000">%s %s</span><span style="color: #800000">'</span>, %<span style="color: #000000">(stAddress,city))
    </span><span style="color: #008000">#</span><span style="color: #008000">利用urlencode函数将字典转为URL可以传递的字符串格式</span>
    url_params=<span style="color: #000000">urllib.urlencode(params)
    </span><span style="color: #008000">#</span><span style="color: #008000">组成完整的URL地址api</span>
    yahooApi=apiStem+<span style="color: #000000">url_params
    </span><span style="color: #008000">#</span><span style="color: #008000">打印该URL地址</span>
    <span style="color: #0000ff">print</span>(<span style="color: #800000">'</span><span style="color: #800000">%s</span><span style="color: #800000">'</span><span style="color: #000000">,yahooApi)
    </span><span style="color: #008000">#</span><span style="color: #008000">打开URL，返回json格式的数据</span>
    c=<span style="color: #000000">urllib.urlopen(yahooApi)
    </span><span style="color: #008000">#</span><span style="color: #008000">返回json解析后的数据字典</span>
    <span style="color: #0000ff">return</span><span style="color: #000000"> json.load(c.read())

</span><span style="color: #0000ff">from</span> time <span style="color: #0000ff">import</span><span style="color: #000000"> sleep
</span><span style="color: #008000">#</span><span style="color: #008000">具体文本数据批量地址经纬度获取函数</span>
<span style="color: #0000ff">def</span><span style="color: #000000"> massPlaceFind(fileName):
    </span><span style="color: #008000">#</span><span style="color: #008000">新建一个可写的文本文件，存储地址，城市，经纬度等信息</span>
    fw=open(<span style="color: #800000">'</span><span style="color: #800000">places.txt</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">wb+</span><span style="color: #800000">'</span><span style="color: #000000">)
    </span><span style="color: #008000">#</span><span style="color: #008000">遍历文本的每一行</span>
    <span style="color: #0000ff">for</span> line <span style="color: #0000ff">in</span><span style="color: #000000"> open(fileName).readlines();
        </span><span style="color: #008000">#</span><span style="color: #008000">去除首尾空格</span>
        line =<span style="color: #000000">line.strip()
        </span><span style="color: #008000">#</span><span style="color: #008000">按tab键分隔开</span>
        lineArr=line.split(<span style="color: #800000">'</span><span style="color: #800000">\t</span><span style="color: #800000">'</span><span style="color: #000000">)
        </span><span style="color: #008000">#</span><span style="color: #008000">利用获取经纬度函数获取该地址经纬度</span>
        retDict=geoGrab(lineArr[1],lineArr[2<span style="color: #000000">])
        </span><span style="color: #008000">#</span><span style="color: #008000">如果错误编码为0，表示没有错误，获取到相应经纬度</span>
        <span style="color: #0000ff">if</span> retDict[<span style="color: #800000">'</span><span style="color: #800000">ResultSet</span><span style="color: #800000">'</span>][<span style="color: #800000">'</span><span style="color: #800000">Error</span><span style="color: #800000">'</span>]==<span style="color: #000000">0:
            </span><span style="color: #008000">#</span><span style="color: #008000">从字典中获取经度</span>
            lat=float(retDict[<span style="color: #800000">'</span><span style="color: #800000">ResultSet</span><span style="color: #800000">'</span>][<span style="color: #800000">'</span><span style="color: #800000">Results</span><span style="color: #800000">'</span>][0][<span style="color: #800000">'</span><span style="color: #800000">latitute</span><span style="color: #800000">'</span><span style="color: #000000">])
            </span><span style="color: #008000">#</span><span style="color: #008000">维度</span>
            lng=float(retDict[<span style="color: #800000">'</span><span style="color: #800000">ResultSet</span><span style="color: #800000">'</span>][<span style="color: #800000">'</span><span style="color: #800000">Results</span><span style="color: #800000">'</span>][0][<span style="color: #800000">'</span><span style="color: #800000">longitute</span><span style="color: #800000">'</span><span style="color: #000000">])
            </span><span style="color: #008000">#</span><span style="color: #008000">打印地名及对应的经纬度信息</span>
            <span style="color: #0000ff">print</span>(<span style="color: #800000">'</span><span style="color: #800000">%s\t%f\t%f</span><span style="color: #800000">'</span>,%<span style="color: #000000">(lineArr[0],lat,lng))
            </span><span style="color: #008000">#</span><span style="color: #008000">将上面的信息存入新的文件中</span>
            fw.write(<span style="color: #800000">'</span><span style="color: #800000">%s\t%f\t%f\n</span><span style="color: #800000">'</span>,%<span style="color: #000000">(line,lat,lng))
        </span><span style="color: #008000">#</span><span style="color: #008000">如果错误编码不为0，打印提示信息</span>
        <span style="color: #0000ff">else</span>:<span style="color: #0000ff">print</span>(<span style="color: #800000">'</span><span style="color: #800000">error fetching</span><span style="color: #800000">'</span><span style="color: #000000">)
        </span><span style="color: #008000">#</span><span style="color: #008000">为防止频繁调用API，造成请求被封，使函数调用延迟一秒</span>
        sleep(1<span style="color: #000000">)
    </span><span style="color: #008000">#</span><span style="color: #008000">文本写入关闭</span>
    fw.close()</pre>
</div>
<p>　　在上述代码中，首先创建一个字典，字典里面存储的是通过URL获取经纬度所必要的参数，即我们想要的返回的数据格式flogs=J；获取数据的appid；以及要输入的地址信息(stAddress,city)。然后，通过urlencode()函数帮助我们将字典类型的信息转化为URL可以传递的字符串格式。最后，打开URL获取返回的JSON类型数据，通过JSON工具来解析返回的数据。且在返回的结果中，当错误编码为0时表示，得到了经纬度信息，而为其他值时，则表示返回经纬度信息失败。</p>
<p>　　此外，在代码中，每次获取完一个地点的经纬度信息后，延迟一秒钟。这样做的目的是为了避免频繁的调用API，请求被封掉的情况。</p>
<p>　　<img src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616184755446-1070216538.jpg" alt=""></p>
<p>2 对地理位置进行聚类</p>
<p>　　我们已经得到了各个地点的经纬度信息，但是我们还要选择计算距离的合适的方式。我们知道，在北极每走几米的经度变化可能达到数十度，而沿着赤道附近走相同的距离，带来的经度变化可能是零。这是，我们可以使用球面余弦定理来计算两个经纬度之间的实际距离。具体代码如下：</p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000">球面距离计算及簇绘图函数</span>
<span style="color: #0000ff">def</span><span style="color: #000000"> distSLC(vecA,vecB):
    </span><span style="color: #008000">#</span><span style="color: #008000">sin()和cos()以弧度未输入，将float角度数值转为弧度，即*pi/180</span>
    a=sin(vecA[0,1]*pi/180)*sin(vecB[0,1]*pi/180<span style="color: #000000">)
    b</span>=cos(vecA[0,1]*pi/180)*cos(vecB[0,1]*pi/180)*<span style="color: #000000">\
        cos(pi</span>*(vecB[0,0]-vecA[0,0])/180<span style="color: #000000">)
    </span><span style="color: #0000ff">return</span> arcos(a+b)*6371.0

<span style="color: #0000ff">import</span><span style="color: #000000"> matplotlib
</span><span style="color: #0000ff">import</span><span style="color: #000000"> matplotlib.pyplot as plt

</span><span style="color: #008000">#</span><span style="color: #008000">@numClust：聚类个数，默认为5</span>
<span style="color: #0000ff">def</span> clusterClubs(numClust=5<span style="color: #000000">):
    datList</span>=<span style="color: #000000">[]
    </span><span style="color: #008000">#</span><span style="color: #008000">解析文本数据中的每一行中的数据特征值</span>
    <span style="color: #0000ff">for</span> line <span style="color: #0000ff">in</span> open(<span style="color: #800000">'</span><span style="color: #800000">places.txt</span><span style="color: #800000">'</span><span style="color: #000000">).readlines():
        lineArr</span>=line.split(<span style="color: #800000">'</span><span style="color: #800000">\t</span><span style="color: #800000">'</span><span style="color: #000000">)
        datList.append([float(lineArr[</span>4]),float(lineArr[4<span style="color: #000000">])])
        datMat</span>=<span style="color: #000000">mat(datList)
        </span><span style="color: #008000">#</span><span style="color: #008000">利用2-均值聚类算法进行聚类</span>
        myCentroids,clusterAssing=<span style="color: #000000">biKmeans(datMat,numClust,\
            distMeas</span>=<span style="color: #000000">distSLC)
        </span><span style="color: #008000">#</span><span style="color: #008000">对聚类结果进行绘图</span>
        fig=<span style="color: #000000">plt.figure()
        rect</span>=[0.1,0.1,0.8,0.8<span style="color: #000000">]
        scatterMarkers</span>=[<span style="color: #800000">'</span><span style="color: #800000">s</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">o</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">^</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">8</span><span style="color: #800000">'</span>.<span style="color: #800000">'</span><span style="color: #800000">p</span><span style="color: #800000">'</span><span style="color: #000000">,\
            </span><span style="color: #800000">'</span><span style="color: #800000">d</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">v</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">h</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">&gt;</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">&lt;</span><span style="color: #800000">'</span><span style="color: #000000">]
        axprops</span>=dict(xticks=[],ytick=<span style="color: #000000">[])
        ax0</span>=fig.add_axes(rect,label=<span style="color: #800000">'</span><span style="color: #800000">ax0</span><span style="color: #800000">'</span>,**<span style="color: #000000">axprops)
        imgP</span>=plt.imread(<span style="color: #800000">'</span><span style="color: #800000">Portland.png</span><span style="color: #800000">'</span><span style="color: #000000">)
        ax0.imshow(imgP)
        ax1</span>=fig.add_axes(rect,label=<span style="color: #800000">'</span><span style="color: #800000">ax1</span><span style="color: #800000">'</span>,frameon=<span style="color: #000000">False)
        </span><span style="color: #0000ff">for</span> i <span style="color: #0000ff">in</span><span style="color: #000000"> range(numClust):
            ptsInCurrCluster</span>=datMat[nonzero(clusterAssing[:,0].A==<span style="color: #000000">i)[0],:]
            markerStyle</span>=scatterMarkers[i %<span style="color: #000000"> len(scatterMarkers))]
            ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0],\
                ptsInCurrCluster[:,</span>1<span style="color: #000000">].flatten().A[0],\
                    marker</span>=markerStyle,s=90<span style="color: #000000">)
        ax1.scatter(myCentroids[:,0].flatten().A[0],\
            myCentroids[:,</span>1].flatten().A[0],marker=<span style="color: #800000">'</span><span style="color: #800000">+</span><span style="color: #800000">'</span>,s=300<span style="color: #000000">)
        </span><span style="color: #008000">#</span><span style="color: #008000">绘制结果显示</span>
        plt.show()</pre>
</div>
<p>最后，将聚类的结果绘制出来：</p>
<p><img src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616190244946-269507925.jpg" alt=""><img style="display: block; margin-left: auto; margin-right: auto" src="https://images2015.cnblogs.com/blog/1134385/201706/1134385-20170616190256181-1822094421.jpg" alt="" width="383" height="274"></p>
<p>五，小结</p>
<p>　　1 聚类是一种无监督的学习方法。聚类区别于分类，即事先不知道要寻找的内容，没有预先设定好的目标变量。</p>
<p>　　2 聚类将数据点归到多个簇中，其中相似的数据点归为同一簇，而不相似的点归为不同的簇。相似度的计算方法有很多，具体的应用选择合适的相似度计算方法</p>
<p>　　3 K-means聚类算法，是一种广泛使用的聚类算法，其中k是需要指定的参数，即需要创建的簇的数目，K-means算法中的k个簇的质心可以通过随机的方式获得，但是这些点需要位于数据范围内。在算法中，计算每个点到质心得距离，选择距离最小的质心对应的簇作为该数据点的划分，然后再基于该分配过程后更新簇的质心。重复上述过程，直至各个簇的质心不再变化为止。</p>
<p>　　4 K-means算法虽然有效，但是容易受到初始簇质心的情况而影响，有可能陷入局部最优解。为了解决这个问题，可以使用另外一种称为二分K-means的聚类算法。二分K-means算法首先将所有数据点分为一个簇；然后使用K-means（k=2）对其进行划分；下一次迭代时，选择使得SSE下降程度最大的簇进行划分；重复该过程，直至簇的个数达到指定的数目为止。实验表明，二分K-means算法的聚类效果要好于普通的K-means聚类算法。</p></div><div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory"></div>
<div id="EntryTag"></div>
<div id="blog_post_info">
</div>
<div class="clear"></div>
<div id="post_next_prev"></div>
</div>


		</div>
		<div class = "postDesc">posted @ <span id="post-date">2017-06-16 19:32</span> <a href='https://www.cnblogs.com/zy230530/'>笨鸟多学</a> 阅读(<span id="post_view_count">...</span>) 评论(<span id="post_comment_count">...</span>)  <a href ="https://i.cnblogs.com/EditPosts.aspx?postid=7029025" rel="nofollow">编辑</a> <a href="#" onclick="AddToWz(7029025);return false;">收藏</a></div>
	</div>
	<script type="text/javascript">var allowComments=true,cb_blogId=343941,cb_entryId=7029025,cb_blogApp=currentBlogApp,cb_blogUserGuid='2649156f-1412-e711-845c-ac853d9f53ac',cb_entryCreatedDate='2017/6/16 19:32:00';loadViewCount(cb_entryId);var cb_postType=1;var isMarkdown=false;</script>
	
</div><!--end: topics 文章、评论容器-->
</div><a name="!comments"></a><div id="blog-comments-placeholder"></div><script type="text/javascript">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>
<div id='comment_form' class='commentform'>
<a name='commentform'></a>
<div id='divCommentShow'></div>
<div id='comment_nav'><span id='span_refresh_tips'></span><a href='javascript:void(0);' onclick='return RefreshCommentList();' id='lnk_RefreshComments' runat='server' clientidmode='Static'>刷新评论</a><a href='#' onclick='return RefreshPage();'>刷新页面</a><a href='#top'>返回顶部</a></div>
<div id='comment_form_container'></div>
<div class='ad_text_commentbox' id='ad_text_under_commentbox'></div>
<div id='ad_t2'></div>
<div id='opt_under_post'></div>
<script async='async' src='https://www.googletagservices.com/tag/js/gpt.js'></script>
<script>
  var googletag = googletag || {};
  googletag.cmd = googletag.cmd || [];
</script>
<script>
  googletag.cmd.push(function() {
        googletag.defineSlot('/1090369/C1', [300, 250], 'div-gpt-ad-1546353474406-0').addService(googletag.pubads());
        googletag.defineSlot('/1090369/C2', [468, 60], 'div-gpt-ad-1539008685004-0').addService(googletag.pubads());
        googletag.pubads().enableSingleRequest();
        googletag.enableServices();
  });
</script>
<div id='cnblogs_c1' class='c_ad_block'>
    <div id='div-gpt-ad-1546353474406-0' style='height:250px; width:300px;'></div>
</div>
<div id='under_post_news'></div>
<div id='cnblogs_c2' class='c_ad_block'>
    <div id='div-gpt-ad-1539008685004-0' style='height:60px; width:468px;'></div>
</div>
<div id='under_post_kb'></div>
<div id='HistoryToday' class='c_ad_block'></div>
<script type='text/javascript'>
 if(enablePostBottom()) {
    codeHighlight();
    fixPostBody();
    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
    deliverT2();
    deliverC1();
    deliverC2();    
    loadNewsAndKb();
    loadBlogSignature();
    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
    loadOptUnderPost();
    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);  
}
</script>
</div>

    
	</div><!--end: forFlow -->
	</div><!--end: mainContent 主体内容容器-->

	<div id="sideBar">
		<div id="sideBarMain">
			
<!--done-->
<div class="newsItem">
<h3 class="catListTitle">公告</h3>
	<div id="blog-news"></div><script type="text/javascript">loadBlogNews();</script>
</div>

			<div id="calendar"><div id="blog-calendar" style="display:none"></div><script type="text/javascript">loadBlogDefaultCalendar();</script></div>
			
			<div id="leftcontentcontainer">
				<div id="blog-sidecolumn"></div><script type="text/javascript">loadBlogSideColumn();</script>
			</div>
			
		</div><!--end: sideBarMain -->
	</div><!--end: sideBar 侧边栏容器 -->
	<div class="clear"></div>
	</div><!--end: main -->
	<div class="clear"></div>
	<div id="footer">
		
<!--done-->
Copyright &copy;2019 笨鸟多学
	</div><!--end: footer -->
</div><!--end: home 自定义的最大容器 -->

</body>
</html>

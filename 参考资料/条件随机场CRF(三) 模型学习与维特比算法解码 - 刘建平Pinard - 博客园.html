<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="referrer" content="origin" />
    <title>条件随机场CRF(三) 模型学习与维特比算法解码 - 刘建平Pinard - 博客园</title>
<meta property="og:description" content="条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三)&#160;模型学习与维特比算法解码 在CRF系列的前两篇，我们总结了CR" />
    <link type="text/css" rel="stylesheet" href="/bundles/blog-common.css?v=-duj5vpGTntb85GJoM3iRI972XwWcI-j8zmqDzyfu2w1"/>
<link id="MainCss" type="text/css" rel="stylesheet" href="/skins/BlackLowKey/bundle-BlackLowKey.css?v=porUb1GRMsPCuLZTJKjoDyrJre6Y7-Oiq-zx-_VcGG81"/>
<link type="text/css" rel="stylesheet" href="/blog/customcss/311024.css?v=YY%2fB9GiizlktSBml7X95KXekn%2bg%3d"/>
<link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/BlackLowKey/bundle-BlackLowKey-mobile.css?v=Go8Bj0ZU9gZXuPSKWcSQabps3E1njgJKYz4DiOq6-N41"/>
    <link title="RSS" type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/pinard/rss"/>
    <link title="RSD" type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/pinard/rsd.xml"/>
<link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/pinard/wlwmanifest.xml"/>
    <script src="//common.cnblogs.com/scripts/jquery-2.2.0.min.js"></script>
    <script>var currentBlogId=311024;var currentBlogApp='pinard',cb_enable_mathjax=true;var isLogined=false;</script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processClass: 'math', processEscapes: true },
        TeX: { 
            equationNumbers: { autoNumber: ['AMS'], useLabelIds: true }, 
            extensions: ['extpfeil.js'],
            Macros: {bm: "\\boldsymbol"}
        },
        'HTML-CSS': { linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic: true } }
        });
    </script><script src="//mathjax.cnblogs.com/2_7_2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/bundles/blog-common.js?v=8XwPau4_W3R9_33N9oT6O2s_wBwLVeOw65HXVt3t7Kk1" type="text/javascript"></script>
</head>
<body>
<a name="top"></a>

<!--PageBeginHtml Block Begin-->
<a class="git-link" href="https://github.com/ljpzzz/machinelearning"></a>
<!--PageBeginHtml Block End-->

<!--done-->
<div id="home">
<div id="header">
	<div id="blogTitle">
	<a id="lnkBlogLogo" href="https://www.cnblogs.com/pinard/"><img id="blogLogo" src="/Skins/custom/images/logo.gif" alt="返回主页" /></a>			
		
<!--done-->
<h1><a id="Header1_HeaderTitle" class="headermaintitle" href="https://www.cnblogs.com/pinard/">刘建平Pinard</a></h1>
<h2>十年码农，对数学统计学，数据挖掘，机器学习，大数据平台，大数据平台应用开发，大数据可视化感兴趣。</h2>



		
	</div><!--end: blogTitle 博客的标题和副标题 -->
	<div id="navigator">
		
<ul id="navList">
<li><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">博客园</a></li>
<li><a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/pinard/">首页</a></li>
<li><a id="blog_nav_newpost" class="menu" rel="nofollow" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">新随笔</a></li>
<li><a id="blog_nav_contact" class="menu" rel="nofollow" href="https://msg.cnblogs.com/send/%E5%88%98%E5%BB%BA%E5%B9%B3Pinard">联系</a></li>
<li><a id="blog_nav_rss" class="menu" href="https://www.cnblogs.com/pinard/rss">订阅</a>
<!--<a id="blog_nav_rss_image" class="aHeaderXML" href="https://www.cnblogs.com/pinard/rss"><img src="//www.cnblogs.com/images/xml.gif" alt="订阅" /></a>--></li>
<li><a id="blog_nav_admin" class="menu" rel="nofollow" href="https://i.cnblogs.com/">管理</a></li>
</ul>
		<div class="blogStats">
			
			<div id="blog_stats">
<span id="stats_post_count">随笔 - 126&nbsp; </span>
<span id="stats_article_count">文章 - 0&nbsp; </span>
<span id="stats-comment_count">评论 - 5012</span>
</div>
			
		</div><!--end: blogStats -->
	</div><!--end: navigator 博客导航栏 -->
</div><!--end: header 头部 -->

<div id="main">
	<div id="mainContent">
	<div class="forFlow">
		
        <div id="post_detail">
<!--done-->
<div id="topics">
	<div class = "post">
		<h1 class = "postTitle">
			<a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/pinard/p/7068574.html">条件随机场CRF(三) 模型学习与维特比算法解码</a>
		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body"><p>　　　　<a id="homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/7048333.html">条件随机场CRF(一)从随机场到线性链条件随机场</a></p>
<p>　　　　<a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/pinard/p/7055072.html">条件随机场CRF(二) 前向后向算法评估标记序列概率</a></p>
<p>　　　　条件随机场CRF(三)&nbsp;模型学习与维特比算法解码</p>
<p>　　　　在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问题是模型参数学习的问题，第三个问题是维特比算法解码的问题。</p>
<h1>1.&nbsp;linear-CRF模型参数学习思路</h1>
<p>　　　　在linear-CRF模型参数学习问题中，我们给定训练数据集$X$和对应的标记序列$Y$，$K$个特征函数$f_k(x,y)$，需要学习linear-CRF的模型参数$w_k$和条件概率$P_w(y|x)$，其中条件概率$P_w(y|x)$和模型参数$w_k$满足一下关系：$$P_w(y|x) = P(y|x) = &nbsp;\frac{1}{Z_w(x)}exp\sum\limits_{k=1}^Kw_kf_k(x,y) =&nbsp;&nbsp;\frac{exp\sum\limits_{k=1}^Kw_kf_k(x,y)}{\sum\limits_{y}exp\sum\limits_{k=1}^Kw_kf_k(x,y)}$$</p>
<p>　　　　所以我们的目标就是求出所有的模型参数$w_k$，这样条件概率$P_w(y|x)$可以从上式计算出来。</p>
<p>　　　　求解这个问题有很多思路，比如梯度下降法，牛顿法，拟牛顿法。同时，这个模型中$P_w(y|x)$的表达式和<a id="ArchiveMonth1_Days_ctl00_Entries_TitleUrl_6" class="entrylistItemTitle" href="http://www.cnblogs.com/pinard/p/6093948.html">最大熵模型原理小结</a>中的模型一样，也可以使用最大熵模型中使用的改进的迭代尺度法(improved iterative scaling, IIS)来求解。</p>
<p>　　　　下面我们只简要介绍用梯度下降法的求解思路。</p>
<h1>2.&nbsp;linear-CRF模型参数学习之梯度下降法求解</h1>
<p>　　　　在使用梯度下降法求解模型参数之前，我们需要定义我们的优化函数，一般极大化条件分布$P_w(y|x)$的对数似然函数如下：$$L(w)= &nbsp;log\prod_{x,y}P_w(y|x)^{\overline{P}(x,y)} = \sum\limits_{x,y}\overline{P}(x,y)logP_w(y|x)$$</p>
<p>　　　　其中$\overline{P}(x,y)$为经验分布，可以从先验知识和训练集样本中得到,这点和最大熵模型类似。为了使用梯度下降法，我们现在极小化$f(w) = -L(P_w)$如下：$$\begin{align}f(w) &amp; = -\sum\limits_{x,y}\overline{P}(x,y)logP_w(y|x) \\ &amp;= &nbsp;\sum\limits_{x,y}\overline{P}(x,y)logZ_w(x) - \sum\limits_{x,y}\overline{P}(x,y)\sum\limits_{k=1}^Kw_kf_k(x,y) \\&amp; = &nbsp;\sum\limits_{x}\overline{P}(x)logZ_w(x) - \sum\limits_{x,y}\overline{P}(x,y)\sum\limits_{k=1}^Kw_kf_k(x,y) \\&amp; = &nbsp;\sum\limits_{x}\overline{P}(x)log\sum\limits_{y}exp\sum\limits_{k=1}^Kw_kf_k(x,y) - \sum\limits_{x,y}\overline{P}(x,y)\sum\limits_{k=1}^Kw_kf_k(x,y) &nbsp;\end{align}$$</p>
<p>　　　　对$w$求导可以得到：$$\frac{\partial f(w)}{\partial w} = \sum\limits_{x,y}\overline{P}(x)P_w(y|x)f(x,y) - &nbsp;\sum\limits_{x,y}\overline{P}(x,y)f(x,y)$$</p>
<p>　　　　有了$w$的导数表达书，就可以用梯度下降法来迭代求解最优的$w$了。注意在迭代过程中，每次更新$w$后，需要同步更新$P_w(x,y)$,以用于下一次迭代的梯度计算。</p>
<p>　　　　梯度下降法的过程这里就不累述了，如果不熟悉梯度下降算法过程建议阅读之前写的<a id="ArchiveMonth1_Days_ctl00_Entries_TitleUrl_7" class="entrylistItemTitle" href="http://www.cnblogs.com/pinard/p/5970503.html">梯度下降（Gradient Descent）小结</a>。以上就是linear-CRF模型参数学习之梯度下降法求解思路总结。</p>
<h1>3.&nbsp;linear-CRF模型维特比算法解码思路</h1>
<p>　　　　现在我们来看linear-CRF的第三个问题：解码。在这个问题中，给定条件随机场的条件概率$P(y|x)$和一个观测序列$x$,要求出满足$P(y|x)$最大的序列$y$。</p>
<p>　　　　这个解码算法最常用的还是和HMM解码类似的维特比算法。到目前为止，我已经在三个地方讲到了维特比算法，第一个是<a id="ArchiveMonth1_Days_ctl00_Entries_TitleUrl_4" class="entrylistItemTitle" href="http://www.cnblogs.com/pinard/p/6677078.html">文本挖掘的分词原理</a>中用于中文分词，第二个是<a id="ArchiveMonth1_Days_ctl00_Entries_TitleUrl_3" class="entrylistItemTitle" href="http://www.cnblogs.com/pinard/p/6991852.html">隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列</a>中用于HMM解码。第三个就是这一篇了。</p>
<p>　　　　维特比算法本身是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。对于具体不同的问题，仅仅是这两个局部状态的定义和对应的递推公式不同而已。由于在之前已详述维特比算法，这里就是做一个简略的流程描述。</p>
<p>　　　　对于我们linear-CRF中的维特比算法，我们的第一个局部状态定义为$\delta_i(l)$,表示在位置$i$标记$l$各个可能取值(1,2...m)对应的非规范化概率的最大值。之所以用非规范化概率是，规范化因子$Z(x)$不影响最大值的比较。根据$\delta_i(l)$的定义，我们递推在位置$i+1$标记$l$的表达式为：$$\delta_{i+1}(l) = \max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\;, l=1,2,...m$$</p>
<p>　　　　和HMM的维特比算法类似，我们需要用另一个局部状态$\Psi_{i+1}(l)$来记录使$\delta_{i+1}(l)$达到最大的位置$i$的标记取值,这个值用来最终回溯最优解，$\Psi_{i+1}(l)$的递推表达式为：$$\Psi_{i+1}(l) = arg\;\max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\; ,l=1,2,...m$$</p>
<h1>4.&nbsp;linear-CRF模型维特比算法流程</h1>
<p>　　　　现在我们总结下&nbsp;linear-CRF模型维特比算法流程：</p>
<p>　　　　输入：模型的$K$个特征函数，和对应的K个权重。观测序列$x=(x_1,x_2,...x_n)$,可能的标记个数$m$</p>
<p>　　　　输出：最优标记序列$y^* =(y_1^*,y_2^*,...y_n^*)$</p>
<p>　　　　1) 初始化：$$\delta_{1}(l) =&nbsp;\sum\limits_{k=1}^Kw_kf_k(y_{0} =start,y_{1} = l,x,i)\}\;, l=1,2,...m $$$$\Psi_{1}(l) = start\;, l=1,2,...m $$</p>
<p>　　　　2) 对于$i=1,2...n-1$,进行递推：$$\delta_{i+1}(l) = \max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\;, l=1,2,...m$$$$\Psi_{i+1}(l) = arg\;\max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\; ,l=1,2,...m$$　　　　</p>
<p>　　　　3) 终止：$$y_n^* = arg\;\max_{1 \leq j \leq m}\delta_n(j)$$</p>
<p>　　　　4)回溯：$$y_i^* =&nbsp;\Psi_{i+1}(y_{i+1}^*)\;, i=n-1,n-2,...1$$</p>
<p>　　　　最终得到最优标记序列$y^* =(y_1^*,y_2^*,...y_n^*)$</p>
<h1>5.&nbsp;linear-CRF模型维特比算法实例</h1>
<p>　　　　下面用一个具体的例子来描述&nbsp;linear-CRF模型维特比算法，例子的模型和CRF系列第一篇中一样，都来源于《统计学习方法》。</p>
<p>　　　　假设输入的都是三个词的句子，即$X=(X_1,X_2,X_3)$,输出的词性标记为$Y=(Y_1,Y_2,Y_3)$,其中$Y \in \{1(名词)，2(动词)\}$</p>
<p>　　　　这里只标记出取值为1的特征函数如下：$$t_1 =t_1(y_{i-1} = 1, y_i =2,x,i), i =2,3,\;\;\lambda_1=1 $$</p>
<p>$$t_2 =t_2(y_1=1,y_2=1,x,2)\;\;\lambda_2=0.6$$</p>
<p>$$t_3 =t_3(y_2=2,y_3=1,x,3)\;\;\lambda_3=1$$</p>
<p>$$t_4 =t_4(y_1=2,y_2=1,x,2)\;\;\lambda_4=1$$</p>
<p>$$t_5 =t_5(y_2=2,y_3=2,x,3)\;\;\lambda_5=0.2$$</p>
<p>$$s_1 =s_1(y_1=1,x,1)\;\;\mu_1 =1$$</p>
<p>$$s_2 =s_2( y_i =2,x,i), i =1,2,\;\;\mu_2=0.5$$</p>
<p>$$s_3 =s_3( y_i =1,x,i), i =2,3,\;\;\mu_3=0.8$$</p>
<p>$$s_4 =s_4(y_3=2,x,3)\;\;\mu_4 =0.5$$</p>
<p>　　　　求标记(1,2,2)的最可能的标记序列。</p>
<p>　　　　首先初始化:$$\delta_1(1) = \mu_1s_1 = 1\;\;\;\delta_1(2) = \mu_2s_2 = 0.5\;\;\;\Psi_{1}(1) =\Psi_{1}(2) = start&nbsp;$$</p>
<p>　　　　接下来开始递推，先看位置2的：</p>
<p>$$\delta_2(1) = max\{\delta_1(1) + t_2\lambda_2+\mu_3s_3, \delta_1(2) + t_4\lambda_4+\mu_3s_3 \} = max\{1+0.6+0.8,0.5+1+0.8\} =2.4\;\;\;\Psi_{2}(1) =1$$</p>
<p>$$\delta_2(2) = max\{\delta_1(1) + t_1\lambda_1+\mu_2s_2,&nbsp;\delta_1(2) + \mu_2s_2\} = max\{1+1+0.5,0.5+0.5\} =2.5\;\;\;\Psi_{2}(2) =1$$</p>
<p>　　　　再看位置3的：</p>
<p>$$\delta_3(1) = max\{\delta_2(1) +\mu_3s_3, \delta_2(2) + t_3\lambda_3+\mu_3s_3\} = max\{2.4+0.8,2.5+1+0.8\} =4.3$$$$\Psi_{3}(1) =2$$</p>
<p>$$\delta_3(2) = max\{\delta_2(1) +t_1\lambda_1 + \mu_4s_4, \delta_2(2) + t_5\lambda_5+\mu_4s_4\} = max\{2.4+1+0.5,2.5+0.2+0.5\} =3.9$$$$\Psi_{3}(2) =1$$</p>
<p>　　　　最终得到$y_3^* =\arg\;max\{\delta_3(1), \delta_3(2)\}$,递推回去，得到：$$y_2^* = \Psi_3(1) =2\;\;y_1^* = \Psi_2(2) =1 $$</p>
<p>　　　　即最终的结果为$(1,2,1)$,即标记为(名词，动词，名词)。</p>
<h1>6.linear-CRF&nbsp;vs HMM</h1>
<p>　　　　linear-CRF模型和HMM模型有很多相似之处，尤其是其三个典型问题非常类似，除了模型参数学习的问题求解方法不同以外，概率估计问题和解码问题使用的算法思想基本也是相同的。同时，两者都可以用于序列模型，因此都广泛用于自然语言处理的各个方面。</p>
<p>　　　　现在来看看两者的不同点。最大的不同点是linear-CRF模型是判别模型，而HMM是生成模型，即linear-CRF模型要优化求解的是条件概率$P(y|x)$,则HMM要求解的是联合分布$P(x,y)$。第二，linear-CRF是利用最大熵模型的思路去建立条件概率模型，对于观测序列并没有做马尔科夫假设。而HMM是在对观测序列做了马尔科夫假设的前提下建立联合分布的模型。</p>
<p>　　　　最后想说的是，只有linear-CRF模型和HMM模型才是可以比较讨论的。但是linear-CRF是CRF的一个特例，CRF本身是一个可以适用于很复杂条件概率的模型，因此理论上CRF的使用范围要比HMM广泛的多。</p>
<p>　　　　以上就是CRF系列的所有内容。</p>
<p>&nbsp;</p>
<p>&nbsp;（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）&nbsp;</p>
<p>&nbsp;</p></div><div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory"></div>
<div id="EntryTag"></div>
<div id="blog_post_info">
</div>
<div class="clear"></div>
<div id="post_next_prev"></div>
</div>


		</div>
		<div class = "postDesc">posted @ <span id="post-date">2017-06-23 15:10</span> <a href='https://www.cnblogs.com/pinard/'>刘建平Pinard</a> 阅读(<span id="post_view_count">...</span>) 评论(<span id="post_comment_count">...</span>)  <a href ="https://i.cnblogs.com/EditPosts.aspx?postid=7068574" rel="nofollow">编辑</a> <a href="#" onclick="AddToWz(7068574);return false;">收藏</a></div>
	</div>
	<script type="text/javascript">var allowComments=true,cb_blogId=311024,cb_entryId=7068574,cb_blogApp=currentBlogApp,cb_blogUserGuid='7d95b75d-b891-e611-845c-ac853d9f53ac',cb_entryCreatedDate='2017/6/23 15:10:00';loadViewCount(cb_entryId);var cb_postType=1;var isMarkdown=false;</script>
	
</div><!--end: topics 文章、评论容器-->
</div><a name="!comments"></a><div id="blog-comments-placeholder"></div><script type="text/javascript">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>
<div id='comment_form' class='commentform'>
<a name='commentform'></a>
<div id='divCommentShow'></div>
<div id='comment_nav'><span id='span_refresh_tips'></span><a href='javascript:void(0);' onclick='return RefreshCommentList();' id='lnk_RefreshComments' runat='server' clientidmode='Static'>刷新评论</a><a href='#' onclick='return RefreshPage();'>刷新页面</a><a href='#top'>返回顶部</a></div>
<div id='comment_form_container'></div>
<div class='ad_text_commentbox' id='ad_text_under_commentbox'></div>
<div id='ad_t2'></div>
<div id='opt_under_post'></div>
<script async='async' src='https://www.googletagservices.com/tag/js/gpt.js'></script>
<script>
  var googletag = googletag || {};
  googletag.cmd = googletag.cmd || [];
</script>
<script>
  googletag.cmd.push(function() {
        googletag.defineSlot('/1090369/C1', [300, 250], 'div-gpt-ad-1546353474406-0').addService(googletag.pubads());
        googletag.defineSlot('/1090369/C2', [468, 60], 'div-gpt-ad-1539008685004-0').addService(googletag.pubads());
        googletag.pubads().enableSingleRequest();
        googletag.enableServices();
  });
</script>
<div id='cnblogs_c1' class='c_ad_block'>
    <div id='div-gpt-ad-1546353474406-0' style='height:250px; width:300px;'></div>
</div>
<div id='under_post_news'></div>
<div id='cnblogs_c2' class='c_ad_block'>
    <div id='div-gpt-ad-1539008685004-0' style='height:60px; width:468px;'></div>
</div>
<div id='under_post_kb'></div>
<div id='HistoryToday' class='c_ad_block'></div>
<script type='text/javascript'>
 if(enablePostBottom()) {
    codeHighlight();
    fixPostBody();
    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
    deliverT2();
    deliverC1();
    deliverC2();    
    loadNewsAndKb();
    loadBlogSignature();
    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
    loadOptUnderPost();
    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);  
}
</script>
</div>

    
	</div><!--end: forFlow -->
	</div><!--end: mainContent 主体内容容器-->

	<div id="sideBar">
		<div id="sideBarMain">
			
<!--done-->
<div class="newsItem">
<h3 class="catListTitle">公告</h3>
	<div id="blog-news"></div><script type="text/javascript">loadBlogNews();</script>
</div>

			<div id="blog-calendar" style="display:none"></div><script type="text/javascript">loadBlogDefaultCalendar();</script>
			
			<div id="leftcontentcontainer">
				<div id="blog-sidecolumn"></div><script type="text/javascript">loadBlogSideColumn();</script>
			</div>
			
		</div><!--end: sideBarMain -->
	</div><!--end: sideBar 侧边栏容器 -->
	<div class="clear"></div>
	</div><!--end: main -->
	<div class="clear"></div>
	<div id="footer">
		
<!--done-->
Copyright &copy;2019 刘建平Pinard
	</div><!--end: footer -->
</div><!--end: home 自定义的最大容器 -->

</body>
</html>
